{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efcb4e6e",
   "metadata": {},
   "source": [
    "# Problema 1\n",
    "\n",
    "Implementar los siguientes métodos de descenso gradiente (naïve = tamaño de paso $\\alpha$ constante):\n",
    "\n",
    "- Descenso gradiente naïve con dirección de descenso aleatoria\n",
    "- Descenso máximo naïve\n",
    "- Descenso gradiente de Newton, con Hessiano exacto\n",
    "- Un método de gradiente conjugado (Fletcher-Reeves, Hestenes-Stiefel, Polak-Ribière)\n",
    "- Método BFGS\n",
    "\n",
    "En cada uno de los métodos, su función debe recibir los siguientes argumentos:\n",
    "\n",
    "- La función objetivo $f$.\n",
    "- El gradiente de la función objetivo $df$.\n",
    "- El hessiano $ddf$ (cuando sea necesario).\n",
    "- Un punto inicial $x_0 \\in \\mathbb{R}^n$.\n",
    "- El tamaño de paso $\\alpha > 0$.\n",
    "- El número máximo de iteraciones $maxIter$.\n",
    "- La tolerancia $\\varepsilon$, así como un criterio de paro.\n",
    "\n",
    "Como resultado, sus algoritmos deben devolver: la mejor solución encontrada *best* (la última de las aproximaciones calculadas); la secuencia de iteraciones $x_k$; la secuencia de valores $f(x_k)$; la secuencia de errores en cada paso (según el error de su criterio de paro).\n",
    "\n",
    "Además, es deseable indicar el número de iteraciones efectuadas por el algoritmo, y si se obtuvo o no convergencia del método."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dbdcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c4ecda",
   "metadata": {},
   "source": [
    "## Descenso gradiente naïve con dirección de descenso aleatoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09eee52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(x, ord=None):\n",
    "    if ord is None:\n",
    "        ord = 2\n",
    "    return np.linalg.norm(x, ord=ord)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a86ab3",
   "metadata": {},
   "source": [
    "Esta función la usaremos para calcular errores con norma midiendo la longitud del vector en un espacio vectorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cebda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def projOrth(u, b_orth):\n",
    "    v = u.copy().astype(float)\n",
    "    if b_orth.ndim == 1:\n",
    "        b = b_orth / (norm(b_orth) + 1e-15)\n",
    "        v -= (v @ b) * b\n",
    "    else:\n",
    "        for b in b_orth:\n",
    "            b = b / (norm(b) + 1e-15)\n",
    "            v -= (v @ b) * b\n",
    "    n = norm(v)\n",
    "    if n < 1e-15:\n",
    "        return v\n",
    "    return v / n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7dffb5",
   "metadata": {},
   "source": [
    "Dado que tenemos el gradiente, debemos ir al negativo ($-\\nabla f(x)$) de este para encontrar una válida dirección de descenso, pero también podemos tomar otras direccioens siempre que formen un ángulo menor de $90°$ con el gradiente.\n",
    "\n",
    "Entonces para generar otras direcciones, se necesita un vector ortogonal al gradiente y combinarlo con un angulo $\\phi$. Esta es la principal función de `projOrth`, con esta función vamos a recibir:\n",
    "\n",
    "- `u`: punto de partida que luego se proyectará (vector).\n",
    "- `b_orth`: esto es un vector al que se quiere ser ortogonal.\n",
    "\n",
    "En caso que b tenga un valor de 1, indica que es un vector; si es mayor será una colección de vectores y queremos proyectar `u` a esto para que sea ortogonal a todos.\n",
    "\n",
    "El proceso es que se normalizará del vector `b_orth` obteniendo un vector unitario `b` luego restamos al vector enviado a su proyección sobre `b` para asegurar que `v` sea ortogonal a `b`:\n",
    "\n",
    "$$\n",
    "v \\leftarrow v - (v \\cdot b) b\n",
    "$$\n",
    "\n",
    "Por último, debemos devolver un vector unitario ortogonal al `b_orth` pero si el vector `u` estaba casi alineado con `b_orth`, entonces al quitarle la proyección se queda en algo casi nulo ($\\|v\\| \\approx 0$), y al normalizar esto ocurrirá un error numérica ($v / \\|v\\|$), por lo que debemos hacer una validación previa para retornar el vector:\n",
    "\n",
    "   * Si $ \\|v\\| \\approx 0$: devolvemos `v` sin normalizar (es decir, un vector casi nulo, aunque no sea unitario).\n",
    "   * Si $ \\|v\\| > 0$: podemos normalizar con seguridad y devolvemos el vector unitario $v / \\|v\\|$.\n",
    "\n",
    "> **Nota:** `v` es una copia del vector de partida `u` para no modificar el valor original enviado.\n",
    "\n",
    "Esto lo utilizamos ya que las direcciones de descenso distintas al gradiente podemos tomar una dirección que forme un ángulo con el gradiente y para esto necesitamos:\n",
    "\n",
    "$$\n",
    "d = \\cos(\\phi)(-\\hat g) + \\sin(\\phi) v\n",
    "$$\n",
    "\n",
    "donde:\n",
    "\n",
    "* $-\\hat g$ es el gradiente negativo unitario.\n",
    "* $v$ es un vector unitario ortogonal al gradiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e83d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescentRandom(\n",
    "    f, df, x0, alpha=0.1, maxIter=500, tol=1e-4, stopCrit=\"grad\",\n",
    "    normOrder=2, isPlottable=True, randomState=None, verbose=False, extra=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Descenso con dirección de descenso aleatoria (o ángulo fijo).\n",
    "    Retorna (best, xs, fxs, errors, metrics).\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(randomState)\n",
    "    extra = extra or {}\n",
    "    phiMode   = extra.get(\"phiMode\", \"random\")\n",
    "    phiFixed  = extra.get(\"phi\", 0.0)\n",
    "    phiRange  = extra.get(\"phiRange\", (-np.pi/2, np.pi/2))\n",
    "\n",
    "    x = np.array(x0, dtype=float).reshape(-1)\n",
    "    n = x.size\n",
    "\n",
    "    xs, fxs, errors = [x.copy()], [f(x)], []\n",
    "    gradNorms, stepNorms, approxErrors, angles, dirs, xs2D, ks = [], [], [], [], [], [], []\n",
    "    t0 = time.time()\n",
    "\n",
    "    g = df(x)\n",
    "    gradNorms.append(norm(g, normOrder))\n",
    "\n",
    "    converged, stopReason = False, \"maxIter\"\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"[k=0] fx={fxs[-1]:.6e} | ||grad||={gradNorms[-1]:.6e} | x={x}\")\n",
    "\n",
    "    for k in range(1, maxIter+1):\n",
    "        # Selección de ángulo\n",
    "        if phiMode == \"fixed\":\n",
    "            phi = float(phiFixed)\n",
    "        else:\n",
    "            lo, hi = phiRange\n",
    "            phi = rng.uniform(lo, hi)\n",
    "\n",
    "        # Dirección de descenso\n",
    "        gnorm = norm(g, normOrder)\n",
    "        if gnorm < 1e-15:\n",
    "            d = -g\n",
    "            angles.append(0.0)\n",
    "        else:\n",
    "            ghat = g / gnorm\n",
    "            z = rng.normal(size=n)\n",
    "            v = projOrth(z, ghat)\n",
    "            if norm(v) < 1e-15:\n",
    "                d = -ghat\n",
    "                phi = 0.0\n",
    "            else:\n",
    "                d = np.cos(phi)*(-ghat) + np.sin(phi)*v\n",
    "\n",
    "        # Paso naïve\n",
    "        x_new = x + alpha * d\n",
    "        fx_new = f(x_new)\n",
    "        step = x_new - x\n",
    "\n",
    "        # Error según stopCrit\n",
    "        if stopCrit == \"grad\":\n",
    "            err = norm(df(x_new), normOrder)\n",
    "        elif stopCrit == \"fx\":\n",
    "            err = abs(fx_new - fxs[-1])\n",
    "        elif stopCrit == \"xAbs\":\n",
    "            err = norm(step, normOrder)\n",
    "        elif stopCrit == \"xRel\":\n",
    "            err = norm(step, normOrder) / max(1.0, norm(x_new, normOrder))\n",
    "        else:\n",
    "            raise ValueError(\"stopCrit inválido.\")\n",
    "\n",
    "        # Registrar historia\n",
    "        xs.append(x_new.copy()); fxs.append(fx_new); errors.append(err)\n",
    "        gradNorms.append(norm(df(x_new), normOrder))\n",
    "        stepNorms.append(norm(step, normOrder))\n",
    "        approxErrors.append(err); angles.append(float(phi)); dirs.append(d.copy()); ks.append(k)\n",
    "        if isPlottable and n == 2:\n",
    "            xs2D.append(x_new.copy())\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"[k={k}] fx={fx_new:.6e} | err({stopCrit})={err:.6e} | \"\n",
    "                  f\"||grad||={gradNorms[-1]:.6e} | ||step||={stepNorms[-1]:.6e} | phi={phi:.3f}\")\n",
    "\n",
    "        # Paro por tolerancia\n",
    "        if err <= tol:\n",
    "            converged, stopReason = True, \"tolerance\"\n",
    "            x = x_new\n",
    "            break\n",
    "\n",
    "        # Avanzar\n",
    "        x, g = x_new, df(x_new)\n",
    "\n",
    "    timeSec = time.time() - t0\n",
    "    best = x\n",
    "    kstar = len(xs) - 1\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"==> stopReason={('tolerance' if converged else 'maxIter')} | \"\n",
    "              f\"iters={kstar} | fx*={fxs[-1]:.6e} | ||grad*||={norm(df(best), normOrder):.6e}\")\n",
    "\n",
    "    metrics = {\n",
    "        \"method\": \"Gradient Descent (random direction)\",\n",
    "        \"converged\": converged,\n",
    "        \"stopReason\": stopReason,\n",
    "        \"iterations\": kstar,\n",
    "        \"finalX\": best.copy(),\n",
    "        \"finalFx\": fxs[-1],\n",
    "        \"gradNorm\": norm(df(best), normOrder),\n",
    "        \"stepNorm\": stepNorms[-1] if stepNorms else None,\n",
    "        \"approxError\": errors[-1] if errors else norm(df(best), normOrder),\n",
    "        \"alpha\": alpha,\n",
    "        \"timeSec\": timeSec,\n",
    "        \"history\": {\n",
    "            \"k\": np.array(ks),\n",
    "            \"gradNorms\": np.array(gradNorms),\n",
    "            \"stepNorms\": np.array(stepNorms),\n",
    "            \"approxErrors\": np.array(approxErrors),\n",
    "            \"angles\": np.array(angles),\n",
    "            \"directions\": np.array(dirs, dtype=float) if len(dirs) else None,\n",
    "            \"xs2D\": np.array(xs2D) if xs2D else None,\n",
    "        },\n",
    "        \"seed\": randomState,\n",
    "    }\n",
    "    return best, np.array(xs), np.array(fxs), np.array(errors), metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abfdede",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21ae202c",
   "metadata": {},
   "source": [
    "## Descenso máximo naïve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067b082c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d918dd8",
   "metadata": {},
   "source": [
    "## Descenso gradiente de Newton, con Hessiano exacto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adab49f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "96526902",
   "metadata": {},
   "source": [
    "## Gradiente conjugado (FR, HS o PR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f83dc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55003e11",
   "metadata": {},
   "source": [
    "## Método BFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69061e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb2b6a91",
   "metadata": {},
   "source": [
    "# Problema 2\n",
    "\n",
    "- Testar sus algoritmos del Ejercicio 1.\n",
    "- Para las funciones 2D, muestre visualizaciones de la secuencia de aproximaciones $\\{x_k\\}$ convergiendo al mínimo local de su función.\n",
    "\n",
    "  ![Ejemplo Gráfica](../images/example_graph.png)\n",
    "\n",
    "- En cada uno de los casos, hallar un tamaño de paso $\\alpha$ que garantice la convergencia de los métodos, y elabore una tabla comparativa de los resultados, error, número de iteraciones requeridas por cada método. Por ejemplo:\n",
    "\n",
    "  | Algoritmo de optimización    | Convergencia (Sí/No) | Número de Iteraciones | Solución | Error |\n",
    "  | ---------------------------- | -------------------- | --------------------- | -------- | ----- |\n",
    "  | Descenso gradiente           |                      |                       |          |       |\n",
    "  | Descenso gradiente aleatorio |                      |                       |          |       |\n",
    "  | Descenso máximo              |                      |                       |          |       |\n",
    "  | Descenso de Newton           |                      |                       |          |       |\n",
    "  | Fletcher-Reeves              |                      |                       |          |       |\n",
    "  | BFGS                         |                      |                       |          |       |\n",
    "\n",
    "- Elabore gráficas que muestren el error de aproximación, en función del número de iteración, y muestre la comparación de la evolución de la convergencia en sus cinco métodos. A partir de estas gráficas, discuta cuál de los métodos es más efectivo, en cada caso. Para ello, debe tomar en cuenta:\n",
    "  - La solución aproximada obtenida\n",
    "  - El error de aproximación\n",
    "  - La norma del gradiente en la solución"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e429a17c",
   "metadata": {},
   "source": [
    "## Inciso a\n",
    "\n",
    "La función $f : \\mathbb{R}^2 \\to \\mathbb{R}$, dada por\n",
    "\n",
    "$$\n",
    "f(x, y) = x^4 + y^4 - 4xy + \\frac{1}{2}y + 1.\n",
    "$$\n",
    "\n",
    "Punto inicial: $x_0 = (-3, 1, -3, 1)^T$, Óptimo: $x^* = (-1.01463, -1.04453)^T, \\; f(x^*) = -1.51132$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c61d401",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "444d9a6f",
   "metadata": {},
   "source": [
    "## Inciso b\n",
    "\n",
    "La función de Rosembrock 2-dimensional $f : \\mathbb{R}^2 \\to \\mathbb{R}$, dada por\n",
    "\n",
    "$$\n",
    "f(x_1, x_2) = 100(x_2 - x_1^2)^2 + (1 - x_1)^2.\n",
    "$$\n",
    "\n",
    "Punto inicial: $x_0 = (-1.2, 1)^T$, Óptimo: $x^* = (1, 1)^T, \\; f(x^*) = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16885ebe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d5be4ab",
   "metadata": {},
   "source": [
    "## Inciso c\n",
    "\n",
    "La función de Rosembrock 7-dimensional $f : \\mathbb{R}^7 \\to \\mathbb{R}$, dada por\n",
    "\n",
    "$$\n",
    "f(x) = \\sum_{i=1}^6 100(x_{i+1} - x_i^2)^2 + (1 - x_i)^2.\n",
    "$$\n",
    "\n",
    "Punto inicial: $x_0 = (-1.2, 1, 1, 1, 1, -1.2, 1)^T$, Óptimo: $x^* = (1, 1, \\dots, 1)^T, \\; f(x^*) = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55d84c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2739ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
