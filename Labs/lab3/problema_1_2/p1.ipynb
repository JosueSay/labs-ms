{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efcb4e6e",
   "metadata": {},
   "source": [
    "# Problema 1\n",
    "\n",
    "Implementar los siguientes métodos de descenso gradiente (naïve = tamaño de paso $\\alpha$ constante):\n",
    "\n",
    "- Descenso gradiente naïve con dirección de descenso aleatoria\n",
    "- Descenso máximo naïve\n",
    "- Descenso gradiente de Newton, con Hessiano exacto\n",
    "- Un método de gradiente conjugado (Fletcher-Reeves, Hestenes-Stiefel, Polak-Ribière)\n",
    "- Método BFGS.\n",
    "\n",
    "En cada uno de los métodos, su función debe recibir los siguientes argumentos:\n",
    "\n",
    "- La función objetivo $f$.\n",
    "- El gradiente de la función objetivo $df$.\n",
    "- El hessiano $ddf$ (cuando sea necesario).\n",
    "- Un punto inicial $x_0 \\in \\mathbb{R}^n$.\n",
    "- El tamaño de paso $\\alpha > 0$.\n",
    "- El número máximo de iteraciones $maxIter$.\n",
    "- La tolerancia $\\varepsilon$, así como un criterio de paro.\n",
    "\n",
    "Como resultado, sus algoritmos deben devolver: la mejor solución encontrada *best* (la última de las aproximaciones calculadas); la secuencia de iteraciones $x_k$; la secuencia de valores $f(x_k)$; la secuencia de errores en cada paso (según el error de su criterio de paro).\n",
    "\n",
    "Además, es deseable indicar el número de iteraciones efectuadas por el algoritmo, y si se obtuvo o no convergencia del método."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edbc38a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4884b07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c867a5e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e92bd7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb2b6a91",
   "metadata": {},
   "source": [
    "# Problema 2\n",
    "\n",
    "- Testar sus algoritmos del Ejercicio 1.\n",
    "- Para las funciones 2D, muestre visualizaciones de la secuencia de aproximaciones $\\{x_k\\}$ convergiendo al mínimo local de su función.\n",
    "\n",
    "  ![Ejemplo Gráfica](../images/example_graph.png)\n",
    "\n",
    "- En cada uno de los casos, hallar un tamaño de paso $\\alpha$ que garantice la convergencia de los métodos, y elabore una tabla comparativa de los resultados, error, número de iteraciones requeridas por cada método. Por ejemplo:\n",
    "\n",
    "  | Algoritmo de optimización    | Convergencia (Sí/No) | Número de Iteraciones | Solución | Error |\n",
    "  | ---------------------------- | -------------------- | --------------------- | -------- | ----- |\n",
    "  | Descenso gradiente           |                      |                       |          |       |\n",
    "  | Descenso gradiente aleatorio |                      |                       |          |       |\n",
    "  | Descenso máximo              |                      |                       |          |       |\n",
    "  | Descenso de Newton           |                      |                       |          |       |\n",
    "  | Fletcher-Reeves              |                      |                       |          |       |\n",
    "  | BFGS                         |                      |                       |          |       |\n",
    "\n",
    "- Elabore gráficas que muestren el error de aproximación, en función del número de iteración, y muestre la comparación de la evolución de la convergencia en sus cinco métodos. A partir de estas gráficas, discuta cuál de los métodos es más efectivo, en cada caso. Para ello, debe tomar en cuenta:\n",
    "  - La solución aproximada obtenida\n",
    "  - El error de aproximación\n",
    "  - La norma del gradiente en la solución"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e429a17c",
   "metadata": {},
   "source": [
    "## Inciso a\n",
    "\n",
    "La función $f : \\mathbb{R}^2 \\to \\mathbb{R}$, dada por\n",
    "\n",
    "$$\n",
    "f(x, y) = x^4 + y^4 - 4xy + \\frac{1}{2}y + 1.\n",
    "$$\n",
    "\n",
    "Punto inicial: $x_0 = (-3, 1, -3, 1)^T$, Óptimo: $x^* = (-1.01463, -1.04453)^T, \\; f(x^*) = -1.51132$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c61d401",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "444d9a6f",
   "metadata": {},
   "source": [
    "## Inciso b\n",
    "\n",
    "La función de Rosembrock 2-dimensional $f : \\mathbb{R}^2 \\to \\mathbb{R}$, dada por\n",
    "\n",
    "$$\n",
    "f(x_1, x_2) = 100(x_2 - x_1^2)^2 + (1 - x_1)^2.\n",
    "$$\n",
    "\n",
    "Punto inicial: $x_0 = (-1.2, 1)^T$, Óptimo: $x^* = (1, 1)^T, \\; f(x^*) = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16885ebe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d5be4ab",
   "metadata": {},
   "source": [
    "## Inciso c\n",
    "\n",
    "La función de Rosembrock 7-dimensional $f : \\mathbb{R}^7 \\to \\mathbb{R}$, dada por\n",
    "\n",
    "$$\n",
    "f(x) = \\sum_{i=1}^6 100(x_{i+1} - x_i^2)^2 + (1 - x_i)^2.\n",
    "$$\n",
    "\n",
    "Punto inicial: $x_0 = (-1.2, 1, 1, 1, 1, -1.2, 1)^T$, Óptimo: $x^* = (1, 1, \\dots, 1)^T, \\; f(x^*) = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55d84c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2739ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
