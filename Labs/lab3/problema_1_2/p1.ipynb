{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efcb4e6e",
   "metadata": {},
   "source": [
    "# Problema 1\n",
    "\n",
    "Implementar los siguientes métodos de descenso gradiente (naïve = tamaño de paso $\\alpha$ constante):\n",
    "\n",
    "- Descenso gradiente naïve con dirección de descenso aleatoria\n",
    "- Descenso máximo naïve\n",
    "- Descenso gradiente de Newton, con Hessiano exacto\n",
    "- Un método de gradiente conjugado (Fletcher-Reeves, Hestenes-Stiefel, Polak-Ribière)\n",
    "- Método BFGS\n",
    "\n",
    "En cada uno de los métodos, su función debe recibir los siguientes argumentos:\n",
    "\n",
    "- La función objetivo $f$.\n",
    "- El gradiente de la función objetivo $df$.\n",
    "- El hessiano $ddf$ (cuando sea necesario).\n",
    "- Un punto inicial $x_0 \\in \\mathbb{R}^n$.\n",
    "- El tamaño de paso $\\alpha > 0$.\n",
    "- El número máximo de iteraciones $maxIter$.\n",
    "- La tolerancia $\\varepsilon$, así como un criterio de paro.\n",
    "\n",
    "Como resultado, sus algoritmos deben devolver: la mejor solución encontrada *best* (la última de las aproximaciones calculadas); la secuencia de iteraciones $x_k$; la secuencia de valores $f(x_k)$; la secuencia de errores en cada paso (según el error de su criterio de paro).\n",
    "\n",
    "Además, es deseable indicar el número de iteraciones efectuadas por el algoritmo, y si se obtuvo o no convergencia del método."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dbdcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d338e331",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 22801"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cb8dc9",
   "metadata": {},
   "source": [
    "> **Nota:** se puede revisar la [documentación](./reporte1.md) para saber los parámetros, retornos, funcionalidad y significado de los algoritmos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d35346",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(x, ord=None):\n",
    "    if ord is None:\n",
    "        ord = 2\n",
    "    return np.linalg.norm(x, ord=ord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f8404e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def projOrth(u, b_orth):\n",
    "    v = u.copy().astype(float)\n",
    "    if b_orth.ndim == 1:\n",
    "        b = b_orth / (norm(b_orth) + 1e-15)\n",
    "        v -= (v @ b) * b\n",
    "    else:\n",
    "        for b in b_orth:\n",
    "            b = b / (norm(b) + 1e-15)\n",
    "            v -= (v @ b) * b\n",
    "    n = norm(v)\n",
    "    if n < 1e-15:\n",
    "        return v\n",
    "    return v / n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c4ecda",
   "metadata": {},
   "source": [
    "## Descenso gradiente naïve con dirección de descenso aleatoria y Descenso máximo naïve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e83d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescentNaive(\n",
    "    f, df, x0, alpha=0.1, maxIter=500, tol=1e-4, stopCrit=\"grad\",\n",
    "    normOrder=2, isPlottable=True, randomState=SEED, verbose=False, extra=None\n",
    "):\n",
    "    \"\"\"\n",
    "    GD naïve con dirección: aleatoria o fija (controlada por extra).\n",
    "    Retorna (best, xs, fxs, errors, metrics).\n",
    "    extra:\n",
    "      - phiMode: \"random\" | \"fixed\"\n",
    "      - phi:     float (rad) si phiMode==\"fixed\"\n",
    "      - phiRange:(lo, hi) rad para muestreo si \"random\" (por defecto (-π/2, π/2))\n",
    "    \"\"\"\n",
    "    # ====== configurar modo/ángulos (merge seguro del extra) ======\n",
    "    base_extra = {\"phiMode\": \"random\", \"phi\": 0.0, \"phiRange\": (-np.pi/2, np.pi/2)}\n",
    "    if extra is not None:\n",
    "        base_extra.update(extra)  # el 'extra' del llamador sobreescribe\n",
    "    phiMode  = base_extra[\"phiMode\"]\n",
    "    phiFixed = float(base_extra[\"phi\"])\n",
    "    lo, hi   = base_extra[\"phiRange\"]\n",
    "\n",
    "    rng = np.random.default_rng(randomState)\n",
    "\n",
    "    x = np.array(x0, dtype=float).reshape(-1)\n",
    "    n = x.size\n",
    "\n",
    "    xs, fxs, errors = [x.copy()], [f(x)], []\n",
    "    gradNorms, stepNorms, approxErrors, angles, dirs, xs2D, ks = [], [], [], [], [], [], []\n",
    "    t0 = time.time()\n",
    "\n",
    "    g = df(x)\n",
    "    gradNorms.append(norm(g, normOrder))\n",
    "\n",
    "    converged, stopReason = False, \"maxIter\"\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"[k=0] fx={fxs[-1]:.6e} | ||grad||={gradNorms[-1]:.6e} | x={x} | mode={phiMode}\")\n",
    "\n",
    "    for k in range(1, maxIter+1):\n",
    "        # --- Selección de ángulo ---\n",
    "        if phiMode == \"fixed\":\n",
    "            phi = phiFixed\n",
    "        else:\n",
    "            phi = rng.uniform(lo, hi)\n",
    "\n",
    "        # --- Dirección de descenso ---\n",
    "        gnorm = norm(g, normOrder)\n",
    "        if gnorm < 1e-15:\n",
    "            d = -g\n",
    "            angles.append(0.0)\n",
    "        else:\n",
    "            ghat = g / gnorm\n",
    "            z = rng.normal(size=n)\n",
    "            v = projOrth(z, ghat)\n",
    "            if norm(v) < 1e-15:\n",
    "                d = -ghat\n",
    "                phi = 0.0\n",
    "            else:\n",
    "                d = np.cos(phi)*(-ghat) + np.sin(phi)*v\n",
    "\n",
    "        # --- Paso naïve ---\n",
    "        x_new = x + alpha * d\n",
    "        fx_new = f(x_new)\n",
    "        step = x_new - x\n",
    "\n",
    "        # --- Error según stopCrit ---\n",
    "        if stopCrit == \"grad\":\n",
    "            err = norm(df(x_new), normOrder)\n",
    "        elif stopCrit == \"fx\":\n",
    "            err = abs(fx_new - fxs[-1])\n",
    "        elif stopCrit == \"xAbs\":\n",
    "            err = norm(step, normOrder)\n",
    "        elif stopCrit == \"xRel\":\n",
    "            err = norm(step, normOrder) / max(1.0, norm(x_new, normOrder))\n",
    "        else:\n",
    "            raise ValueError(\"stopCrit inválido (usa 'grad','fx','xAbs','xRel').\")\n",
    "\n",
    "        # --- Registro ---\n",
    "        xs.append(x_new.copy()); fxs.append(fx_new); errors.append(err)\n",
    "        gradNorms.append(norm(df(x_new), normOrder))\n",
    "        stepNorms.append(norm(step, normOrder))\n",
    "        approxErrors.append(err); angles.append(float(phi)); dirs.append(d.copy()); ks.append(k)\n",
    "        if isPlottable and n == 2:\n",
    "            xs2D.append(x_new.copy())\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"[k={k}] fx={fx_new:.6e} | err({stopCrit})={err:.6e} | \"\n",
    "                  f\"||grad||={gradNorms[-1]:.6e} | ||step||={stepNorms[-1]:.6e} | phi={phi:.3f}\")\n",
    "\n",
    "        # --- Paro por tolerancia ---\n",
    "        if err <= tol:\n",
    "            converged, stopReason = True, \"tolerance\"\n",
    "            x = x_new\n",
    "            break\n",
    "\n",
    "        # --- Avanzar ---\n",
    "        x, g = x_new, df(x_new)\n",
    "\n",
    "    timeSec = time.time() - t0\n",
    "    best = x\n",
    "    kstar = len(xs) - 1\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"==> stopReason={('tolerance' if converged else 'maxIter')} | \"\n",
    "              f\"iters={kstar} | fx*={fxs[-1]:.6e} | ||grad*||={norm(df(best), normOrder):.6e}\")\n",
    "\n",
    "    # Etiqueta de método según modo\n",
    "    method_label = (\"Steepest Descent (naive)\" if (phiMode == \"fixed\" and abs(phiFixed) < 1e-14)\n",
    "                    else \"Gradient Descent (random direction naive)\" if phiMode == \"random\"\n",
    "                    else \"Gradient Descent (fixed-angle naive)\")\n",
    "\n",
    "    metrics = {\n",
    "        \"method\": method_label,\n",
    "        \"converged\": converged,\n",
    "        \"stopReason\": stopReason,\n",
    "        \"iterations\": kstar,\n",
    "        \"finalX\": best.copy(),\n",
    "        \"finalFx\": fxs[-1],\n",
    "        \"gradNorm\": norm(df(best), normOrder),\n",
    "        \"stepNorm\": stepNorms[-1] if stepNorms else None,\n",
    "        \"approxError\": errors[-1] if errors else norm(df(best), normOrder),\n",
    "        \"alpha\": alpha,\n",
    "        \"timeSec\": timeSec,\n",
    "        \"history\": {\n",
    "            \"k\": np.array(ks),\n",
    "            \"gradNorms\": np.array(gradNorms),\n",
    "            \"stepNorms\": np.array(stepNorms),\n",
    "            \"approxErrors\": np.array(approxErrors),\n",
    "            \"angles\": np.array(angles),\n",
    "            \"directions\": np.array(dirs, dtype=float) if len(dirs) else None,\n",
    "            \"xs2D\": np.array(xs2D) if xs2D else None,\n",
    "        },\n",
    "        \"seed\": randomState,\n",
    "    }\n",
    "    return best, np.array(xs), np.array(fxs), np.array(errors), metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046a7272",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescentRandom(\n",
    "    f, df, x0,\n",
    "    alpha=0.1, maxIter=500, tol=1e-4, stopCrit=\"grad\",\n",
    "    normOrder=2, isPlottable=True, randomState=SEED, verbose=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Descenso gradiente naïve con dirección aleatoria.\n",
    "    \"\"\"\n",
    "    extra = {\"phiMode\": \"random\"}  # se arma internamente\n",
    "    return gradientDescentNaive(\n",
    "        f, df, x0,\n",
    "        alpha=alpha, maxIter=maxIter, tol=tol, stopCrit=stopCrit,\n",
    "        normOrder=normOrder, isPlottable=isPlottable,\n",
    "        randomState=randomState, verbose=verbose, extra=extra\n",
    "    )\n",
    "\n",
    "def steepestDescent(\n",
    "    f, df, x0,\n",
    "    alpha=0.1, maxIter=500, tol=1e-4, stopCrit=\"grad\",\n",
    "    normOrder=2, isPlottable=True, randomState=SEED, verbose=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Descenso máximo (steepest) naïve — equivale a ángulo fijo 0\n",
    "    \"\"\"\n",
    "    extra = {\"phiMode\": \"fixed\", \"phi\": 0.0}  # se arma internamente\n",
    "    return gradientDescentNaive(\n",
    "        f, df, x0,\n",
    "        alpha=alpha, maxIter=maxIter, tol=tol, stopCrit=stopCrit,\n",
    "        normOrder=normOrder, isPlottable=isPlottable,\n",
    "        randomState=randomState, verbose=verbose, extra=extra\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d918dd8",
   "metadata": {},
   "source": [
    "## Descenso gradiente de Newton, con Hessiano exacto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adab49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def newtonDescent(\n",
    "    f, df, x0, alpha=1.0, maxIter=100, tol=1e-6, stopCrit=\"grad\",\n",
    "    normOrder=2, isPlottable=True, randomState=SEED, verbose=False, extra=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Método de Newton con Hessiano exacto y paso constante alpha.\n",
    "    Retorna (best, xs, fxs, errors, metrics).\n",
    "\n",
    "    extra:\n",
    "      - ddf: callable Hessiano H(x) o matriz fija (np.ndarray, n x n)\n",
    "      - solveSystem: \"solve\" (default) | \"inv\"\n",
    "    \"\"\"\n",
    "    # ------- Validaciones de 'extra' -------\n",
    "    extra = extra or {}\n",
    "    if \"ddf\" not in extra or extra[\"ddf\"] is None:\n",
    "        raise ValueError(\"newtonDescent requiere extra['ddf'] (Hessiano exacto o callable).\")\n",
    "    ddf = extra[\"ddf\"]\n",
    "    solveSystem = extra.get(\"solveSystem\", \"solve\")\n",
    "    if solveSystem not in (\"solve\", \"inv\"):\n",
    "        raise ValueError(\"extra['solveSystem'] debe ser 'solve' o 'inv'.\")\n",
    "\n",
    "    # ------- Estado inicial -------\n",
    "    x = np.array(x0, dtype=float).reshape(-1)\n",
    "    n = x.size\n",
    "\n",
    "    xs, fxs, errors = [x.copy()], [f(x)], []\n",
    "    gradNorms, stepNorms, approxErrors, dirs, xs2D, ks = [], [], [], [], [], []\n",
    "    t0 = time.time()\n",
    "\n",
    "    g = df(x)\n",
    "    gradNorms.append(norm(g, ord=normOrder))\n",
    "    converged, stopReason = False, \"maxIter\"\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"[k=0] fx={fxs[-1]:.6e} | ||grad||={gradNorms[-1]:.6e} | x={x}\")\n",
    "\n",
    "    # ------- Bucle principal -------\n",
    "    for k in range(1, maxIter + 1):\n",
    "        # Hessiano exacto (callable o matriz fija)\n",
    "        H = ddf(x) if callable(ddf) else np.array(ddf, dtype=float)\n",
    "\n",
    "        # Dirección de Newton: resolver H d = -g (sin invertir si es posible)\n",
    "        try:\n",
    "            if solveSystem == \"solve\":\n",
    "                d = -np.linalg.solve(H, g)\n",
    "            else:  # \"inv\"\n",
    "                Hinv = np.linalg.inv(H)\n",
    "                d = -(Hinv @ g)\n",
    "        except np.linalg.LinAlgError:\n",
    "            # Fallback numérico: pseudo-inversa\n",
    "            d = -(np.linalg.pinv(H) @ g)\n",
    "\n",
    "        # Asegurar dirección de descenso si es posible (g^T d < 0)\n",
    "        gd = float(g @ d)\n",
    "        if not np.isfinite(gd) or gd >= 0:\n",
    "            # Fallback a steepest descent si Newton no da descenso\n",
    "            d = -g\n",
    "            gd = float(g @ d)\n",
    "\n",
    "        # Paso Naïve\n",
    "        x_new = x + alpha * d\n",
    "        fx_new = f(x_new)\n",
    "        step = x_new - x\n",
    "\n",
    "        # Error según stopCrit\n",
    "        if stopCrit == \"grad\":\n",
    "            err = norm(df(x_new), ord=normOrder)\n",
    "        elif stopCrit == \"fx\":\n",
    "            err = abs(fx_new - fxs[-1])\n",
    "        elif stopCrit == \"xAbs\":\n",
    "            err = norm(step, ord=normOrder)\n",
    "        elif stopCrit == \"xRel\":\n",
    "            err = norm(step, ord=normOrder) / max(1.0, norm(x_new, ord=normOrder))\n",
    "        else:\n",
    "            raise ValueError(\"stopCrit inválido (usa 'grad','fx','xAbs','xRel').\")\n",
    "\n",
    "        # Registro\n",
    "        xs.append(x_new.copy()); fxs.append(fx_new); errors.append(err)\n",
    "        gradNorms.append(norm(df(x_new), ord=normOrder))\n",
    "        stepNorms.append(norm(step, ord=normOrder))\n",
    "        approxErrors.append(err); dirs.append(d.copy()); ks.append(k)\n",
    "        if isPlottable and n == 2:\n",
    "            xs2D.append(x_new.copy())\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"[k={k}] fx={fx_new:.6e} | err({stopCrit})={err:.6e} | \"\n",
    "                  f\"||grad||={gradNorms[-1]:.6e} | ||step||={stepNorms[-1]:.6e} | gTd={gd:.3e}\")\n",
    "\n",
    "        # Paro por tolerancia\n",
    "        if err <= tol:\n",
    "            converged, stopReason = True, \"tolerance\"\n",
    "            x = x_new\n",
    "            break\n",
    "\n",
    "        # Avanzar\n",
    "        x, g = x_new, df(x_new)\n",
    "\n",
    "    # ------- Cierre -------\n",
    "    timeSec = time.time() - t0\n",
    "    best = x\n",
    "    kstar = len(xs) - 1\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"==> stopReason={('tolerance' if converged else 'maxIter')} | \"\n",
    "              f\"iters={kstar} | fx*={fxs[-1]:.6e} | ||grad*||={norm(df(best), ord=normOrder):.6e}\")\n",
    "\n",
    "    metrics = {\n",
    "        \"method\": \"Newton (exact Hessian, naive step)\",\n",
    "        \"converged\": converged,\n",
    "        \"stopReason\": stopReason,\n",
    "        \"iterations\": kstar,\n",
    "        \"finalX\": best.copy(),\n",
    "        \"finalFx\": fxs[-1],\n",
    "        \"gradNorm\": norm(df(best), ord=normOrder),\n",
    "        \"stepNorm\": stepNorms[-1] if stepNorms else None,\n",
    "        \"approxError\": errors[-1] if errors else norm(df(best), ord=normOrder),\n",
    "        \"alpha\": alpha,\n",
    "        \"timeSec\": timeSec,\n",
    "        \"history\": {\n",
    "            \"k\": np.array(ks),\n",
    "            \"gradNorms\": np.array(gradNorms),\n",
    "            \"stepNorms\": np.array(stepNorms),\n",
    "            \"approxErrors\": np.array(approxErrors),\n",
    "            \"angles\": None,\n",
    "            \"directions\": np.array(dirs, dtype=float) if len(dirs) else None,\n",
    "            \"xs2D\": np.array(xs2D) if xs2D else None,\n",
    "        },\n",
    "        \"seed\": randomState,\n",
    "        \"solveSystem\": solveSystem,\n",
    "    }\n",
    "    return best, np.array(xs), np.array(fxs), np.array(errors), metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96526902",
   "metadata": {},
   "source": [
    "## Gradiente conjugado (FR, HS o PR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f83dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conjugateGradientDescent(\n",
    "    f, df, x0, alpha=0.1, maxIter=500, tol=1e-4, stopCrit=\"grad\",\n",
    "    normOrder=2, isPlottable=True, randomState=SEED, verbose=False, extra=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Gradiente Conjugado (no lineal) con paso constante (naïve).\n",
    "\n",
    "    extra:\n",
    "      - betaRule: \"FR\" (default) | \"PR\" | \"PR+\" | \"HS\"\n",
    "      - restartEvery: int o None (reinicio periódico a -gradiente)\n",
    "      - denomEps: flotante para estabilizar divisiones (default 1e-15)\n",
    "      - ensureDescent: bool (default True) -> si g^T d>=0, reinicia a -g\n",
    "    \"\"\"\n",
    "    extra = extra or {}\n",
    "    betaRule = extra.get(\"betaRule\", \"FR\").upper()\n",
    "    restartEvery = extra.get(\"restartEvery\", None)\n",
    "    denomEps = float(extra.get(\"denomEps\", 1e-15))\n",
    "    ensureDescent = bool(extra.get(\"ensureDescent\", True))\n",
    "\n",
    "    x = np.array(x0, dtype=float).reshape(-1)\n",
    "    n = x.size\n",
    "\n",
    "    xs, fxs, errors = [x.copy()], [f(x)], []\n",
    "    gradNorms, stepNorms, approxErrors, dirs, betas, xs2D, ks = [], [], [], [], [], [], []\n",
    "    t0 = time.time()\n",
    "\n",
    "    g = df(x)\n",
    "    d = -g.copy()\n",
    "    gradNorms.append(norm(g, normOrder))\n",
    "    converged, stopReason = False, \"maxIter\"\n",
    "    restartCount = 0\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"[k=0] fx={fxs[-1]:.6e} | ||grad||={gradNorms[-1]:.6e} | x={x} | rule={betaRule}\")\n",
    "\n",
    "    for k in range(1, maxIter + 1):\n",
    "        # --- Paso naïve ---\n",
    "        x_new = x + alpha * d\n",
    "        fx_new = f(x_new)\n",
    "        step = x_new - x\n",
    "\n",
    "        # --- Error según stopCrit ---\n",
    "        if stopCrit == \"grad\":\n",
    "            err = norm(df(x_new), normOrder)\n",
    "        elif stopCrit == \"fx\":\n",
    "            err = abs(fx_new - fxs[-1])\n",
    "        elif stopCrit == \"xAbs\":\n",
    "            err = norm(step, normOrder)\n",
    "        elif stopCrit == \"xRel\":\n",
    "            err = norm(step, normOrder) / max(1.0, norm(x_new, normOrder))\n",
    "        else:\n",
    "            raise ValueError(\"stopCrit inválido (usa 'grad','fx','xAbs','xRel').\")\n",
    "\n",
    "        # --- Registro ---\n",
    "        xs.append(x_new.copy()); fxs.append(fx_new); errors.append(err)\n",
    "        gradNorms.append(norm(df(x_new), normOrder))\n",
    "        stepNorms.append(norm(step, normOrder))\n",
    "        approxErrors.append(err); dirs.append(d.copy()); ks.append(k)\n",
    "        if isPlottable and n == 2: xs2D.append(x_new.copy())\n",
    "        if verbose:\n",
    "            print(f\"[k={k}] fx={fx_new:.6e} | err({stopCrit})={err:.6e} | \"\n",
    "                  f\"||grad||={gradNorms[-1]:.6e} | ||step||={stepNorms[-1]:.6e}\")\n",
    "\n",
    "        # --- Paro por tolerancia ---\n",
    "        if err <= tol:\n",
    "            converged, stopReason = True, \"tolerance\"\n",
    "            x = x_new\n",
    "            break\n",
    "\n",
    "        # --- Avanzar ---\n",
    "        g_new = df(x_new)\n",
    "        y = g_new - g\n",
    "\n",
    "        # --- β según regla ---\n",
    "        gg = max(denomEps, float(g @ g))\n",
    "        if betaRule == \"FR\":\n",
    "            beta = float(g_new @ g_new) / gg\n",
    "        elif betaRule in (\"PR\", \"PR+\"):\n",
    "            beta_raw = float(g_new @ (g_new - g)) / gg\n",
    "            beta = max(0.0, beta_raw) if betaRule == \"PR+\" else beta_raw\n",
    "        elif betaRule == \"HS\":\n",
    "            denom = float(d @ y)\n",
    "            beta = (float(g_new @ y) / (denom if abs(denom) > denomEps else np.sign(denom)*denomEps))\n",
    "        else:\n",
    "            raise ValueError(\"extra['betaRule'] debe ser 'FR'|'PR'|'PR+'|'HS'.\")\n",
    "\n",
    "        # --- Posible reinicio periódico ---\n",
    "        if isinstance(restartEvery, int) and restartEvery > 0 and (k % restartEvery == 0):\n",
    "            beta = 0.0\n",
    "            restartCount += 1\n",
    "\n",
    "        # --- Nueva dirección ---\n",
    "        d_new = -g_new + beta * d\n",
    "\n",
    "        # --- Asegurar descenso si se solicita ---\n",
    "        if ensureDescent:\n",
    "            gtd = float(g_new @ d_new)\n",
    "            if (not np.isfinite(gtd)) or (gtd >= 0):\n",
    "                d_new = -g_new\n",
    "                restartCount += 1\n",
    "\n",
    "        betas.append(beta)\n",
    "\n",
    "        # Actualizar estado\n",
    "        x, g, d = x_new, g_new, d_new\n",
    "\n",
    "    # --- Cierre ---\n",
    "    timeSec = time.time() - t0\n",
    "    best = x\n",
    "    kstar = len(xs) - 1\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"==> stopReason={('tolerance' if converged else 'maxIter')} | \"\n",
    "              f\"iters={kstar} | fx*={fxs[-1]:.6e} | ||grad*||={norm(df(best), normOrder):.6e} | \"\n",
    "              f\"restarts={restartCount}\")\n",
    "\n",
    "    method_label = f\"Nonlinear Conjugate Gradient (naive, {betaRule})\"\n",
    "    metrics = {\n",
    "        \"method\": method_label,\n",
    "        \"converged\": converged,\n",
    "        \"stopReason\": stopReason,\n",
    "        \"iterations\": kstar,\n",
    "        \"finalX\": best.copy(),\n",
    "        \"finalFx\": fxs[-1],\n",
    "        \"gradNorm\": norm(df(best), normOrder),\n",
    "        \"stepNorm\": stepNorms[-1] if stepNorms else None,\n",
    "        \"approxError\": errors[-1] if errors else norm(df(best), normOrder),\n",
    "        \"alpha\": alpha,\n",
    "        \"timeSec\": timeSec,\n",
    "        \"history\": {\n",
    "            \"k\": np.array(ks),\n",
    "            \"gradNorms\": np.array(gradNorms),\n",
    "            \"stepNorms\": np.array(stepNorms),\n",
    "            \"approxErrors\": np.array(approxErrors),\n",
    "            \"angles\": None,\n",
    "            \"directions\": np.array(dirs, dtype=float) if len(dirs) else None,\n",
    "            \"betas\": np.array(betas) if betas else None,\n",
    "            \"xs2D\": np.array(xs2D) if xs2D else None,\n",
    "        },\n",
    "        \"seed\": randomState,\n",
    "        \"betaRule\": betaRule,\n",
    "        \"restartEvery\": restartEvery,\n",
    "        \"restarts\": restartCount,\n",
    "        \"ensureDescent\": ensureDescent,\n",
    "    }\n",
    "    return best, np.array(xs), np.array(fxs), np.array(errors), metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55003e11",
   "metadata": {},
   "source": [
    "## Método BFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69061e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bfgsDescent(\n",
    "    f, df, x0, alpha=0.1, maxIter=500, tol=1e-6, stopCrit=\"grad\",\n",
    "    normOrder=2, isPlottable=True, randomState=None, verbose=False, extra=None\n",
    "):\n",
    "    \"\"\"\n",
    "    BFGS (quasi-Newton) con paso constante (naive).\n",
    "\n",
    "    extra:\n",
    "      - H0:        np.ndarray (n x n) inversa-Hessiana inicial (default: I)\n",
    "      - skipUpdateIf: float (default: 1e-12)  # umbral para y^T s; si <=, saltar actualización\n",
    "      - ensureDescent: bool (default: True)   # si g^T d >= 0, forzar d = -g\n",
    "    \"\"\"\n",
    "    extra = extra or {}\n",
    "    skipUpdateIf  = float(extra.get(\"skipUpdateIf\", 1e-12))\n",
    "    ensureDescent = bool(extra.get(\"ensureDescent\", True))\n",
    "\n",
    "    # --- Estado inicial ---\n",
    "    x = np.array(x0, dtype=float).reshape(-1)\n",
    "    n = x.size\n",
    "\n",
    "    # H0\n",
    "    if \"H0\" in extra and extra[\"H0\"] is not None:\n",
    "        H = np.array(extra[\"H0\"], dtype=float)\n",
    "        if H.shape != (n, n):\n",
    "            raise ValueError(\"extra['H0'] debe tener shape (n,n).\")\n",
    "    else:\n",
    "        H = np.eye(n, dtype=float)\n",
    "\n",
    "    xs, fxs, errors = [x.copy()], [f(x)], []\n",
    "    gradNorms, stepNorms, approxErrors, dirs, xs2D, ks = [], [], [], [], [], []\n",
    "    t0 = time.time()\n",
    "\n",
    "    g = df(x)\n",
    "    gradNorms.append(norm(g, normOrder))\n",
    "    converged, stopReason = False, \"maxIter\"\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"[k=0] fx={fxs[-1]:.6e} | ||grad||={gradNorms[-1]:.6e} | x={x}\")\n",
    "\n",
    "    # --- Bucle principal ---\n",
    "    for k in range(1, maxIter + 1):\n",
    "        # Dirección BFGS\n",
    "        d = -(H @ g)\n",
    "\n",
    "        if ensureDescent:\n",
    "            gtd = float(g @ d)\n",
    "            if (not np.isfinite(gtd)) or (gtd >= 0):\n",
    "                d = -g  # respaldo\n",
    "\n",
    "        # Paso naïve\n",
    "        x_new = x + alpha * d\n",
    "        fx_new = f(x_new)\n",
    "        step = x_new - x\n",
    "\n",
    "        # Error según stopCrit\n",
    "        if stopCrit == \"grad\":\n",
    "            err = norm(df(x_new), normOrder)\n",
    "        elif stopCrit == \"fx\":\n",
    "            err = abs(fx_new - fxs[-1])\n",
    "        elif stopCrit == \"xAbs\":\n",
    "            err = norm(step, normOrder)\n",
    "        elif stopCrit == \"xRel\":\n",
    "            err = norm(step, normOrder) / max(1.0, norm(x_new, normOrder))\n",
    "        else:\n",
    "            raise ValueError(\"stopCrit inválido (usa 'grad','fx','xAbs','xRel').\")\n",
    "\n",
    "        # Registro\n",
    "        xs.append(x_new.copy()); fxs.append(fx_new); errors.append(err)\n",
    "        gradNorms.append(norm(df(x_new), normOrder))\n",
    "        stepNorms.append(norm(step, normOrder))\n",
    "        approxErrors.append(err); dirs.append(d.copy()); ks.append(k)\n",
    "        if isPlottable and n == 2:\n",
    "            xs2D.append(x_new.copy())\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"[k={k}] fx={fx_new:.6e} | err({stopCrit})={err:.6e} | \"\n",
    "                  f\"||grad||={gradNorms[-1]:.6e} | ||step||={stepNorms[-1]:.6e}\")\n",
    "\n",
    "        # Paro por tolerancia\n",
    "        if err <= tol:\n",
    "            converged, stopReason = True, \"tolerance\"\n",
    "            x = x_new\n",
    "            break\n",
    "\n",
    "        # Preparar actualización BFGS\n",
    "        g_new = df(x_new)\n",
    "        s = x_new - x\n",
    "        y = g_new - g\n",
    "        yTs = float(y @ s)\n",
    "\n",
    "        # Actualización (si hay curvatura suficiente)\n",
    "        if np.isfinite(yTs) and (yTs > skipUpdateIf):\n",
    "            rho = 1.0 / yTs\n",
    "            I = np.eye(n)\n",
    "            # (I - rho s y^T) H (I - rho y s^T) + rho s s^T\n",
    "            V = I - rho * np.outer(s, y)\n",
    "            H = V @ H @ V.T + rho * np.outer(s, s)\n",
    "            # Simetrizar por estabilidad numérica\n",
    "            H = 0.5 * (H + H.T)\n",
    "        # si no, se mantiene H\n",
    "\n",
    "        # Avanzar\n",
    "        x, g = x_new, g_new\n",
    "\n",
    "    # --- Cierre ---\n",
    "    timeSec = time.time() - t0\n",
    "    best = x\n",
    "    kstar = len(xs) - 1\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"==> stopReason={('tolerance' if converged else 'maxIter')} | \"\n",
    "              f\"iters={kstar} | fx*={fxs[-1]:.6e} | ||grad*||={norm(df(best), normOrder):.6e}\")\n",
    "\n",
    "    metrics = {\n",
    "        \"method\": \"BFGS (naive step)\",\n",
    "        \"converged\": converged,\n",
    "        \"stopReason\": stopReason,\n",
    "        \"iterations\": kstar,\n",
    "        \"finalX\": best.copy(),\n",
    "        \"finalFx\": fxs[-1],\n",
    "        \"gradNorm\": norm(df(best), normOrder),\n",
    "        \"stepNorm\": stepNorms[-1] if stepNorms else None,\n",
    "        \"approxError\": errors[-1] if errors else norm(df(best), normOrder),\n",
    "        \"alpha\": alpha,\n",
    "        \"timeSec\": timeSec,\n",
    "        \"history\": {\n",
    "            \"k\": np.array(ks),\n",
    "            \"gradNorms\": np.array(gradNorms),\n",
    "            \"stepNorms\": np.array(stepNorms),\n",
    "            \"approxErrors\": np.array(approxErrors),\n",
    "            \"angles\": None,\n",
    "            \"directions\": np.array(dirs, dtype=float) if len(dirs) else None,\n",
    "            \"xs2D\": np.array(xs2D) if xs2D else None,\n",
    "        },\n",
    "        \"seed\": randomState,\n",
    "        \"skipUpdateIf\": skipUpdateIf,\n",
    "        \"ensureDescent\": ensureDescent,\n",
    "    }\n",
    "    return best, np.array(xs), np.array(fxs), np.array(errors), metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2b6a91",
   "metadata": {},
   "source": [
    "# Problema 2\n",
    "\n",
    "- Testar sus algoritmos del Ejercicio 1.\n",
    "- Para las funciones 2D, muestre visualizaciones de la secuencia de aproximaciones $\\{x_k\\}$ convergiendo al mínimo local de su función.\n",
    "\n",
    "  ![Ejemplo Gráfica](../images/example_graph.png)\n",
    "\n",
    "- Elabore gráficas que muestren el error de aproximación, en función del número de iteración, y muestre la comparación de la evolución de la convergencia en sus cinco métodos. A partir de estas gráficas, discuta cuál de los métodos es más efectivo, en cada caso. Para ello, debe tomar en cuenta:\n",
    "  - La solución aproximada obtenida\n",
    "  - El error de aproximación\n",
    "  - La norma del gradiente en la solución\n",
    "\n",
    "- En cada uno de los casos, hallar un tamaño de paso $\\alpha$ que garantice la convergencia de los métodos, y elabore una tabla comparativa de los resultados, error, número de iteraciones requeridas por cada método. Por ejemplo:\n",
    "\n",
    "  | Algoritmo de optimización    | Convergencia (Sí/No) | Número de Iteraciones | Solución | Error |\n",
    "  | ---------------------------- | -------------------- | --------------------- | -------- | ----- |\n",
    "  | Descenso gradiente           |                      |                       |          |       |\n",
    "  | Descenso gradiente aleatorio |                      |                       |          |       |\n",
    "  | Descenso máximo              |                      |                       |          |       |\n",
    "  | Descenso de Newton           |                      |                       |          |       |\n",
    "  | Fletcher-Reeves              |                      |                       |          |       |\n",
    "  | BFGS                         |                      |                       |          |       |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e429a17c",
   "metadata": {},
   "source": [
    "## Inciso a\n",
    "\n",
    "La función $f : \\mathbb{R}^2 \\to \\mathbb{R}$, dada por\n",
    "\n",
    "$$\n",
    "f(x, y) = x^4 + y^4 - 4xy + \\frac{1}{2}y + 1.\n",
    "$$\n",
    "\n",
    "Punto inicial: $x_0 = (-3, 1, -3, 1)^T$, Óptimo: $x^* = (-1.01463, -1.04453)^T, \\; f(x^*) = -1.51132$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c61d401",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "444d9a6f",
   "metadata": {},
   "source": [
    "## Inciso b\n",
    "\n",
    "La función de Rosembrock 2-dimensional $f : \\mathbb{R}^2 \\to \\mathbb{R}$, dada por\n",
    "\n",
    "$$\n",
    "f(x_1, x_2) = 100(x_2 - x_1^2)^2 + (1 - x_1)^2.\n",
    "$$\n",
    "\n",
    "Punto inicial: $x_0 = (-1.2, 1)^T$, Óptimo: $x^* = (1, 1)^T, \\; f(x^*) = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16885ebe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d5be4ab",
   "metadata": {},
   "source": [
    "## Inciso c\n",
    "\n",
    "La función de Rosembrock 7-dimensional $f : \\mathbb{R}^7 \\to \\mathbb{R}$, dada por\n",
    "\n",
    "$$\n",
    "f(x) = \\sum_{i=1}^6 100(x_{i+1} - x_i^2)^2 + (1 - x_i)^2.\n",
    "$$\n",
    "\n",
    "Punto inicial: $x_0 = (-1.2, 1, 1, 1, 1, -1.2, 1)^T$, Óptimo: $x^* = (1, 1, \\dots, 1)^T, \\; f(x^*) = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55d84c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2739ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
