{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efcb4e6e",
   "metadata": {},
   "source": [
    "# Problema 1\n",
    "\n",
    "Implementar los siguientes métodos de descenso gradiente (naïve = tamaño de paso $\\alpha$ constante):\n",
    "\n",
    "- Descenso gradiente naïve con dirección de descenso aleatoria\n",
    "- Descenso máximo naïve\n",
    "- Descenso gradiente de Newton, con Hessiano exacto\n",
    "- Un método de gradiente conjugado (Fletcher-Reeves, Hestenes-Stiefel, Polak-Ribière)\n",
    "- Método BFGS.\n",
    "\n",
    "En cada uno de los métodos, su función debe recibir los siguientes argumentos:\n",
    "\n",
    "- La función objetivo $f$.\n",
    "- El gradiente de la función objetivo $df$.\n",
    "- El hessiano $ddf$ (cuando sea necesario).\n",
    "- Un punto inicial $x_0 \\in \\mathbb{R}^n$.\n",
    "- El tamaño de paso $\\alpha > 0$.\n",
    "- El número máximo de iteraciones $maxIter$.\n",
    "- La tolerancia $\\varepsilon$, así como un criterio de paro.\n",
    "\n",
    "Como resultado, sus algoritmos deben devolver: la mejor solución encontrada *best* (la última de las aproximaciones calculadas); la secuencia de iteraciones $x_k$; la secuencia de valores $f(x_k)$; la secuencia de errores en cada paso (según el error de su criterio de paro).\n",
    "\n",
    "Además, es deseable indicar el número de iteraciones efectuadas por el algoritmo, y si se obtuvo o no convergencia del método."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
