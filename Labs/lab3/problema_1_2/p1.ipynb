{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efcb4e6e",
   "metadata": {},
   "source": [
    "# Problema 1\n",
    "\n",
    "Implementar los siguientes métodos de descenso gradiente (naïve = tamaño de paso $\\alpha$ constante):\n",
    "\n",
    "- Descenso gradiente naïve con dirección de descenso aleatoria\n",
    "- Descenso máximo naïve\n",
    "- Descenso gradiente de Newton, con Hessiano exacto\n",
    "- Un método de gradiente conjugado (Fletcher-Reeves, Hestenes-Stiefel, Polak-Ribière)\n",
    "- Método BFGS\n",
    "\n",
    "En cada uno de los métodos, su función debe recibir los siguientes argumentos:\n",
    "\n",
    "- La función objetivo $f$.\n",
    "- El gradiente de la función objetivo $df$.\n",
    "- El hessiano $ddf$ (cuando sea necesario).\n",
    "- Un punto inicial $x_0 \\in \\mathbb{R}^n$.\n",
    "- El tamaño de paso $\\alpha > 0$.\n",
    "- El número máximo de iteraciones $maxIter$.\n",
    "- La tolerancia $\\varepsilon$, así como un criterio de paro.\n",
    "\n",
    "Como resultado, sus algoritmos deben devolver: la mejor solución encontrada *best* (la última de las aproximaciones calculadas); la secuencia de iteraciones $x_k$; la secuencia de valores $f(x_k)$; la secuencia de errores en cada paso (según el error de su criterio de paro).\n",
    "\n",
    "Además, es deseable indicar el número de iteraciones efectuadas por el algoritmo, y si se obtuvo o no convergencia del método."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dbdcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d338e331",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 22801"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cb8dc9",
   "metadata": {},
   "source": [
    "> **Nota:** se puede revisar la [documentación](./reporte1.md) para saber los parámetros, retornos, funcionalidad y significado de los algoritmos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d35346",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(x, ord=None):\n",
    "    if ord is None:\n",
    "        ord = 2\n",
    "    return np.linalg.norm(x, ord=ord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f8404e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def projOrth(u, b_orth):\n",
    "    v = u.copy().astype(float)\n",
    "    if b_orth.ndim == 1:\n",
    "        b = b_orth / (norm(b_orth) + 1e-15)\n",
    "        v -= (v @ b) * b\n",
    "    else:\n",
    "        for b in b_orth:\n",
    "            b = b / (norm(b) + 1e-15)\n",
    "            v -= (v @ b) * b\n",
    "    n = norm(v)\n",
    "    if n < 1e-15:\n",
    "        return v\n",
    "    return v / n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c4ecda",
   "metadata": {},
   "source": [
    "## Descenso gradiente naïve con dirección de descenso aleatoria y Descenso máximo naïve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e83d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescentNaive(\n",
    "    f, df, x0, alpha=0.1, maxIter=500, tol=1e-4, stopCrit=\"grad\",\n",
    "    normOrder=2, isPlottable=True, randomState=SEED, verbose=False, extra=None\n",
    "):\n",
    "    \"\"\"\n",
    "    GD naïve con dirección: aleatoria o fija (controlada por extra).\n",
    "    Retorna (best, xs, fxs, errors, metrics).\n",
    "    extra:\n",
    "      - phiMode: \"random\" | \"fixed\"\n",
    "      - phi:     float (rad) si phiMode==\"fixed\"\n",
    "      - phiRange:(lo, hi) rad para muestreo si \"random\" (por defecto (-π/2, π/2))\n",
    "    \"\"\"\n",
    "    # ====== configurar modo/ángulos (merge seguro del extra) ======\n",
    "    base_extra = {\"phiMode\": \"random\", \"phi\": 0.0, \"phiRange\": (-np.pi/2, np.pi/2)}\n",
    "    if extra is not None:\n",
    "        base_extra.update(extra)  # el 'extra' del llamador sobreescribe\n",
    "    phiMode  = base_extra[\"phiMode\"]\n",
    "    phiFixed = float(base_extra[\"phi\"])\n",
    "    lo, hi   = base_extra[\"phiRange\"]\n",
    "\n",
    "    rng = np.random.default_rng(randomState)\n",
    "\n",
    "    x = np.array(x0, dtype=float).reshape(-1)\n",
    "    n = x.size\n",
    "\n",
    "    xs, fxs, errors = [x.copy()], [f(x)], []\n",
    "    gradNorms, stepNorms, approxErrors, angles, dirs, xs2D, ks = [], [], [], [], [], [], []\n",
    "    t0 = time.time()\n",
    "\n",
    "    g = df(x)\n",
    "    gradNorms.append(norm(g, normOrder))\n",
    "\n",
    "    converged, stopReason = False, \"maxIter\"\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"[k=0] fx={fxs[-1]:.6e} | ||grad||={gradNorms[-1]:.6e} | x={x} | mode={phiMode}\")\n",
    "\n",
    "    for k in range(1, maxIter+1):\n",
    "        # --- Selección de ángulo ---\n",
    "        if phiMode == \"fixed\":\n",
    "            phi = phiFixed\n",
    "        else:\n",
    "            phi = rng.uniform(lo, hi)\n",
    "\n",
    "        # --- Dirección de descenso ---\n",
    "        gnorm = norm(g, normOrder)\n",
    "        if gnorm < 1e-15:\n",
    "            d = -g\n",
    "            angles.append(0.0)\n",
    "        else:\n",
    "            ghat = g / gnorm\n",
    "            z = rng.normal(size=n)\n",
    "            v = projOrth(z, ghat)\n",
    "            if norm(v) < 1e-15:\n",
    "                d = -ghat\n",
    "                phi = 0.0\n",
    "            else:\n",
    "                d = np.cos(phi)*(-ghat) + np.sin(phi)*v\n",
    "\n",
    "        # --- Paso naïve ---\n",
    "        x_new = x + alpha * d\n",
    "        fx_new = f(x_new)\n",
    "        step = x_new - x\n",
    "\n",
    "        # --- Error según stopCrit ---\n",
    "        if stopCrit == \"grad\":\n",
    "            err = norm(df(x_new), normOrder)\n",
    "        elif stopCrit == \"fx\":\n",
    "            err = abs(fx_new - fxs[-1])\n",
    "        elif stopCrit == \"xAbs\":\n",
    "            err = norm(step, normOrder)\n",
    "        elif stopCrit == \"xRel\":\n",
    "            err = norm(step, normOrder) / max(1.0, norm(x_new, normOrder))\n",
    "        else:\n",
    "            raise ValueError(\"stopCrit inválido (usa 'grad','fx','xAbs','xRel').\")\n",
    "\n",
    "        # --- Registro ---\n",
    "        xs.append(x_new.copy()); fxs.append(fx_new); errors.append(err)\n",
    "        gradNorms.append(norm(df(x_new), normOrder))\n",
    "        stepNorms.append(norm(step, normOrder))\n",
    "        approxErrors.append(err); angles.append(float(phi)); dirs.append(d.copy()); ks.append(k)\n",
    "        if isPlottable and n == 2:\n",
    "            xs2D.append(x_new.copy())\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"[k={k}] fx={fx_new:.6e} | err({stopCrit})={err:.6e} | \"\n",
    "                  f\"||grad||={gradNorms[-1]:.6e} | ||step||={stepNorms[-1]:.6e} | phi={phi:.3f}\")\n",
    "\n",
    "        # --- Paro por tolerancia ---\n",
    "        if err <= tol:\n",
    "            converged, stopReason = True, \"tolerance\"\n",
    "            x = x_new\n",
    "            break\n",
    "\n",
    "        # --- Avanzar ---\n",
    "        x, g = x_new, df(x_new)\n",
    "\n",
    "    timeSec = time.time() - t0\n",
    "    best = x\n",
    "    kstar = len(xs) - 1\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"==> stopReason={('tolerance' if converged else 'maxIter')} | \"\n",
    "              f\"iters={kstar} | fx*={fxs[-1]:.6e} | ||grad*||={norm(df(best), normOrder):.6e}\")\n",
    "\n",
    "    # Etiqueta de método según modo\n",
    "    method_label = (\"Steepest Descent (naive)\" if (phiMode == \"fixed\" and abs(phiFixed) < 1e-14)\n",
    "                    else \"Gradient Descent (random direction naive)\" if phiMode == \"random\"\n",
    "                    else \"Gradient Descent (fixed-angle naive)\")\n",
    "\n",
    "    metrics = {\n",
    "        \"method\": method_label,\n",
    "        \"converged\": converged,\n",
    "        \"stopReason\": stopReason,\n",
    "        \"iterations\": kstar,\n",
    "        \"finalX\": best.copy(),\n",
    "        \"finalFx\": fxs[-1],\n",
    "        \"gradNorm\": norm(df(best), normOrder),\n",
    "        \"stepNorm\": stepNorms[-1] if stepNorms else None,\n",
    "        \"approxError\": errors[-1] if errors else norm(df(best), normOrder),\n",
    "        \"alpha\": alpha,\n",
    "        \"timeSec\": timeSec,\n",
    "        \"history\": {\n",
    "            \"k\": np.array(ks),\n",
    "            \"gradNorms\": np.array(gradNorms),\n",
    "            \"stepNorms\": np.array(stepNorms),\n",
    "            \"approxErrors\": np.array(approxErrors),\n",
    "            \"angles\": np.array(angles),\n",
    "            \"directions\": np.array(dirs, dtype=float) if len(dirs) else None,\n",
    "            \"xs2D\": np.array(xs2D) if xs2D else None,\n",
    "        },\n",
    "        \"seed\": randomState,\n",
    "    }\n",
    "    return best, np.array(xs), np.array(fxs), np.array(errors), metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046a7272",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescentRandom(\n",
    "    f, df, x0,\n",
    "    alpha=0.1, maxIter=500, tol=1e-4, stopCrit=\"grad\",\n",
    "    normOrder=2, isPlottable=True, randomState=SEED, verbose=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Descenso gradiente naïve con dirección aleatoria.\n",
    "    \"\"\"\n",
    "    extra = {\"phiMode\": \"random\"}  # se arma internamente\n",
    "    return gradientDescentNaive(\n",
    "        f, df, x0,\n",
    "        alpha=alpha, maxIter=maxIter, tol=tol, stopCrit=stopCrit,\n",
    "        normOrder=normOrder, isPlottable=isPlottable,\n",
    "        randomState=randomState, verbose=verbose, extra=extra\n",
    "    )\n",
    "\n",
    "def steepestDescent(\n",
    "    f, df, x0,\n",
    "    alpha=0.1, maxIter=500, tol=1e-4, stopCrit=\"grad\",\n",
    "    normOrder=2, isPlottable=True, randomState=SEED, verbose=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Descenso máximo (steepest) naïve — equivale a ángulo fijo 0\n",
    "    \"\"\"\n",
    "    extra = {\"phiMode\": \"fixed\", \"phi\": 0.0}  # se arma internamente\n",
    "    return gradientDescentNaive(\n",
    "        f, df, x0,\n",
    "        alpha=alpha, maxIter=maxIter, tol=tol, stopCrit=stopCrit,\n",
    "        normOrder=normOrder, isPlottable=isPlottable,\n",
    "        randomState=randomState, verbose=verbose, extra=extra\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d918dd8",
   "metadata": {},
   "source": [
    "## Descenso gradiente de Newton, con Hessiano exacto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adab49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def newtonDescent(\n",
    "    f, df, x0, alpha=1.0, maxIter=100, tol=1e-6, stopCrit=\"grad\",\n",
    "    normOrder=2, isPlottable=True, randomState=SEED, verbose=False, extra=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Método de Newton con Hessiano exacto y paso constante alpha.\n",
    "    Retorna (best, xs, fxs, errors, metrics).\n",
    "\n",
    "    extra:\n",
    "      - ddf: callable Hessiano H(x) o matriz fija (np.ndarray, n x n)\n",
    "      - solveSystem: \"solve\" (default) | \"inv\"\n",
    "    \"\"\"\n",
    "    # ------- Validaciones de 'extra' -------\n",
    "    extra = extra or {}\n",
    "    if \"ddf\" not in extra or extra[\"ddf\"] is None:\n",
    "        raise ValueError(\"newtonDescent requiere extra['ddf'] (Hessiano exacto o callable).\")\n",
    "    ddf = extra[\"ddf\"]\n",
    "    solveSystem = extra.get(\"solveSystem\", \"solve\")\n",
    "    if solveSystem not in (\"solve\", \"inv\"):\n",
    "        raise ValueError(\"extra['solveSystem'] debe ser 'solve' o 'inv'.\")\n",
    "\n",
    "    # ------- Estado inicial -------\n",
    "    x = np.array(x0, dtype=float).reshape(-1)\n",
    "    n = x.size\n",
    "\n",
    "    xs, fxs, errors = [x.copy()], [f(x)], []\n",
    "    gradNorms, stepNorms, approxErrors, dirs, xs2D, ks = [], [], [], [], [], []\n",
    "    t0 = time.time()\n",
    "\n",
    "    g = df(x)\n",
    "    gradNorms.append(np.linalg.norm(g, ord=normOrder))\n",
    "    converged, stopReason = False, \"maxIter\"\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"[k=0] fx={fxs[-1]:.6e} | ||grad||={gradNorms[-1]:.6e} | x={x}\")\n",
    "\n",
    "    # ------- Bucle principal -------\n",
    "    for k in range(1, maxIter + 1):\n",
    "        # Hessiano exacto (callable o matriz fija)\n",
    "        H = ddf(x) if callable(ddf) else np.array(ddf, dtype=float)\n",
    "\n",
    "        # Dirección de Newton: resolver H d = -g (sin invertir si es posible)\n",
    "        try:\n",
    "            if solveSystem == \"solve\":\n",
    "                d = -np.linalg.solve(H, g)\n",
    "            else:  # \"inv\"\n",
    "                Hinv = np.linalg.inv(H)\n",
    "                d = -(Hinv @ g)\n",
    "        except np.linalg.LinAlgError:\n",
    "            # Fallback numérico: pseudo-inversa\n",
    "            d = -(np.linalg.pinv(H) @ g)\n",
    "\n",
    "        # Asegurar dirección de descenso si es posible (g^T d < 0)\n",
    "        gd = float(g @ d)\n",
    "        if not np.isfinite(gd) or gd >= 0:\n",
    "            # Fallback a steepest descent si Newton no da descenso\n",
    "            d = -g\n",
    "            gd = float(g @ d)\n",
    "\n",
    "        # Paso Naïve\n",
    "        x_new = x + alpha * d\n",
    "        fx_new = f(x_new)\n",
    "        step = x_new - x\n",
    "\n",
    "        # Error según stopCrit\n",
    "        if stopCrit == \"grad\":\n",
    "            err = np.linalg.norm(df(x_new), ord=normOrder)\n",
    "        elif stopCrit == \"fx\":\n",
    "            err = abs(fx_new - fxs[-1])\n",
    "        elif stopCrit == \"xAbs\":\n",
    "            err = np.linalg.norm(step, ord=normOrder)\n",
    "        elif stopCrit == \"xRel\":\n",
    "            err = np.linalg.norm(step, ord=normOrder) / max(1.0, np.linalg.norm(x_new, ord=normOrder))\n",
    "        else:\n",
    "            raise ValueError(\"stopCrit inválido (usa 'grad','fx','xAbs','xRel').\")\n",
    "\n",
    "        # Registro\n",
    "        xs.append(x_new.copy()); fxs.append(fx_new); errors.append(err)\n",
    "        gradNorms.append(np.linalg.norm(df(x_new), ord=normOrder))\n",
    "        stepNorms.append(np.linalg.norm(step, ord=normOrder))\n",
    "        approxErrors.append(err); dirs.append(d.copy()); ks.append(k)\n",
    "        if isPlottable and n == 2:\n",
    "            xs2D.append(x_new.copy())\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"[k={k}] fx={fx_new:.6e} | err({stopCrit})={err:.6e} | \"\n",
    "                  f\"||grad||={gradNorms[-1]:.6e} | ||step||={stepNorms[-1]:.6e} | gTd={gd:.3e}\")\n",
    "\n",
    "        # Paro por tolerancia\n",
    "        if err <= tol:\n",
    "            converged, stopReason = True, \"tolerance\"\n",
    "            x = x_new\n",
    "            break\n",
    "\n",
    "        # Avanzar\n",
    "        x, g = x_new, df(x_new)\n",
    "\n",
    "    # ------- Cierre -------\n",
    "    timeSec = time.time() - t0\n",
    "    best = x\n",
    "    kstar = len(xs) - 1\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"==> stopReason={('tolerance' if converged else 'maxIter')} | \"\n",
    "              f\"iters={kstar} | fx*={fxs[-1]:.6e} | ||grad*||={np.linalg.norm(df(best), ord=normOrder):.6e}\")\n",
    "\n",
    "    metrics = {\n",
    "        \"method\": \"Newton (exact Hessian, naive step)\",\n",
    "        \"converged\": converged,\n",
    "        \"stopReason\": stopReason,\n",
    "        \"iterations\": kstar,\n",
    "        \"finalX\": best.copy(),\n",
    "        \"finalFx\": fxs[-1],\n",
    "        \"gradNorm\": np.linalg.norm(df(best), ord=normOrder),\n",
    "        \"stepNorm\": stepNorms[-1] if stepNorms else None,\n",
    "        \"approxError\": errors[-1] if errors else np.linalg.norm(df(best), ord=normOrder),\n",
    "        \"alpha\": alpha,\n",
    "        \"timeSec\": timeSec,\n",
    "        \"history\": {\n",
    "            \"k\": np.array(ks),\n",
    "            \"gradNorms\": np.array(gradNorms),\n",
    "            \"stepNorms\": np.array(stepNorms),\n",
    "            \"approxErrors\": np.array(approxErrors),\n",
    "            \"angles\": None,\n",
    "            \"directions\": np.array(dirs, dtype=float) if len(dirs) else None,\n",
    "            \"xs2D\": np.array(xs2D) if xs2D else None,\n",
    "        },\n",
    "        \"seed\": randomState,\n",
    "        \"solveSystem\": solveSystem,\n",
    "    }\n",
    "    return best, np.array(xs), np.array(fxs), np.array(errors), metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96526902",
   "metadata": {},
   "source": [
    "## Gradiente conjugado (FR, HS o PR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f83dc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55003e11",
   "metadata": {},
   "source": [
    "## Método BFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69061e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb2b6a91",
   "metadata": {},
   "source": [
    "# Problema 2\n",
    "\n",
    "- Testar sus algoritmos del Ejercicio 1.\n",
    "- Para las funciones 2D, muestre visualizaciones de la secuencia de aproximaciones $\\{x_k\\}$ convergiendo al mínimo local de su función.\n",
    "\n",
    "  ![Ejemplo Gráfica](../images/example_graph.png)\n",
    "\n",
    "- Elabore gráficas que muestren el error de aproximación, en función del número de iteración, y muestre la comparación de la evolución de la convergencia en sus cinco métodos. A partir de estas gráficas, discuta cuál de los métodos es más efectivo, en cada caso. Para ello, debe tomar en cuenta:\n",
    "  - La solución aproximada obtenida\n",
    "  - El error de aproximación\n",
    "  - La norma del gradiente en la solución\n",
    "\n",
    "- En cada uno de los casos, hallar un tamaño de paso $\\alpha$ que garantice la convergencia de los métodos, y elabore una tabla comparativa de los resultados, error, número de iteraciones requeridas por cada método. Por ejemplo:\n",
    "\n",
    "  | Algoritmo de optimización    | Convergencia (Sí/No) | Número de Iteraciones | Solución | Error |\n",
    "  | ---------------------------- | -------------------- | --------------------- | -------- | ----- |\n",
    "  | Descenso gradiente           |                      |                       |          |       |\n",
    "  | Descenso gradiente aleatorio |                      |                       |          |       |\n",
    "  | Descenso máximo              |                      |                       |          |       |\n",
    "  | Descenso de Newton           |                      |                       |          |       |\n",
    "  | Fletcher-Reeves              |                      |                       |          |       |\n",
    "  | BFGS                         |                      |                       |          |       |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e429a17c",
   "metadata": {},
   "source": [
    "## Inciso a\n",
    "\n",
    "La función $f : \\mathbb{R}^2 \\to \\mathbb{R}$, dada por\n",
    "\n",
    "$$\n",
    "f(x, y) = x^4 + y^4 - 4xy + \\frac{1}{2}y + 1.\n",
    "$$\n",
    "\n",
    "Punto inicial: $x_0 = (-3, 1, -3, 1)^T$, Óptimo: $x^* = (-1.01463, -1.04453)^T, \\; f(x^*) = -1.51132$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c61d401",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "444d9a6f",
   "metadata": {},
   "source": [
    "## Inciso b\n",
    "\n",
    "La función de Rosembrock 2-dimensional $f : \\mathbb{R}^2 \\to \\mathbb{R}$, dada por\n",
    "\n",
    "$$\n",
    "f(x_1, x_2) = 100(x_2 - x_1^2)^2 + (1 - x_1)^2.\n",
    "$$\n",
    "\n",
    "Punto inicial: $x_0 = (-1.2, 1)^T$, Óptimo: $x^* = (1, 1)^T, \\; f(x^*) = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16885ebe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d5be4ab",
   "metadata": {},
   "source": [
    "## Inciso c\n",
    "\n",
    "La función de Rosembrock 7-dimensional $f : \\mathbb{R}^7 \\to \\mathbb{R}$, dada por\n",
    "\n",
    "$$\n",
    "f(x) = \\sum_{i=1}^6 100(x_{i+1} - x_i^2)^2 + (1 - x_i)^2.\n",
    "$$\n",
    "\n",
    "Punto inicial: $x_0 = (-1.2, 1, 1, 1, 1, -1.2, 1)^T$, Óptimo: $x^* = (1, 1, \\dots, 1)^T, \\; f(x^*) = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55d84c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2739ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
